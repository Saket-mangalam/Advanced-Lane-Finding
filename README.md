{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced Lane Finding**\n",
    "\n",
    "## **Udacity Self Driving Car Engineer Nanodegree - Project 2**\n",
    "\n",
    "The goal of this project is to develop a pipeline to process a video stream from a forward-facing camera mounted on the front of a car, and output an annotated video which identifies:\n",
    "\n",
    "* The positions of the lane lines\n",
    "* The location of the vehicle relative to the center of the lane\n",
    "* The radius of curvature of the road\n",
    "\n",
    "This can be done using advanced computer vision and colorspace exploration. The important steps of the project include -\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "## Description:\n",
    "\n",
    "### Step 1: Camera calibration and Image Undistortion\n",
    "\n",
    "The first step in the project is to remove any distortion from the images by calculating the camera calibration matrix and distortion coefficients using a series of images of a chessboard.\n",
    "\n",
    "\n",
    "#### * Camera Calibration using chessboard images\n",
    "#### * Image undistortion using image points and object points obtained \n",
    "\n",
    "Since opencv function calibrateCamera only works on grayscale images, so first convert the image to grayscale and use it to calculate undistortion matrix for conversion. Using this matrix we can undistort any colorspace image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/undistortion.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "The difference is very subtle to note, but it exists.\n",
    "\n",
    "### Step 2: Keypoint detection\n",
    "\n",
    "Keypoint detection using opencv SIFT helps in calculating the points og contrast and color differences i.e corners in an image. I tried using keypoint detection to identify the top and bottom of lane lines and then use them to warp the image perspective. \n",
    "\n",
    "However, this increases the computation of the pipeline and with ambiguous goals. We can use the hyperparameters for warping the image and it does a good task. \n",
    "\n",
    "However, on a road with very steep elevation the hyperparameter fitting might fail to gve good results, so this can be a good substitute way to find the points required for perspective transform.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/Keypoint_img.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Step 3: Perspective transform\n",
    "\n",
    "Perspective transform is used to change the perspective or viewpoint of an image. OpenCV provides fuctions for this purpose.\n",
    "\n",
    "Here we want to change the image perspective from front view to a bird's eye view so that the features of lane line can be read easily.\n",
    "\n",
    "As per the requirement of image processing we need to transform the image such that it covers only the lane area of image. For this we need 4 points on the original image and where we want them to be in the warped view. I used:\n",
    "\n",
    "```python\n",
    "# define source and destination points for transform\n",
    "    src = np.float32([(575,464),\n",
    "                      (707,464), \n",
    "                      (258,682), \n",
    "                      (1049,682)])\n",
    "    dst = np.float32([(450,0),\n",
    "                      (w-450,0),\n",
    "                      (450,h),\n",
    "                      (w-450,h)])\n",
    "```\n",
    "where w and h are width = 720 and height = 1280 of image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/perspective.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Step 4: Color masking and scale conversion\n",
    "\n",
    "This step is implemented to remove all the noise from an image using different color channels and color thresholds and just keep the relevant information intact in the image (in this case the lane lines). Following steps were taken :\n",
    "\n",
    "#### RGB to HLS Conversion\n",
    "\n",
    "RGB to HLS conversion of an image helped in the previous project on lane finding, so tried it. Does not give good results on light condition variances in this project. However, L-channel of HLS colorspace within the range `[220,255]` reads white lines precisely with varying light conditions.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/hls_image_thresholded.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### RGB to LAB Conversion\n",
    "\n",
    "LAB is another such color space with the B-channel within the range `[190,255]` detects white lines almost accurately.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/lab_image_thresholded.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### Other conversions\n",
    "\n",
    "HSV, YCrCb colorspces were also tested for image masking. Best fit was obtained using the above 2 channels. Others were discarded.\n",
    "\n",
    "*White Yellow Thresholding*\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/wy_image_thresholded.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "*YCrCb Image*\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/ycrcb_image.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "*HLS Image*\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/hls_image.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "*HSV Image*\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/hsv_image.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "*LAB Image*\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/lab_image.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### Final Conversion\n",
    "\n",
    "Results after white and yellow color masking using HLS-L and LAB-B channels is visualized.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/hls_lab_combination.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "\n",
    "### Step 5: GrayScale conversion\n",
    "\n",
    "This conversion is pretty good for gradient calculation and edge detections. But I didnot use it later in my pipeline because my image was already single channel binary format.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/grayscaled.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "### Step 6: Sobel transformation\n",
    "\n",
    "Sobel transform of combined_img. Sobel Magnitude threshold and Sobel Gradient threshold were considered to efficiently remove noise. I implemented sobel transform in following way:\n",
    "\n",
    "```python\n",
    "    #sobel x and y transforms\n",
    "    sobelx = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize= kernel)\n",
    "    sobely = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize= kernel)\n",
    "    # absolute value of transforms\n",
    "    abs_sobelx = np.absolute(sobelx)\n",
    "    abs_sobely = np.absolute(sobely)\n",
    "    # magnitude of transforms\n",
    "    mag_sobel = np.sqrt(abs_sobelx**2 + abs_sobely**2)\n",
    "    # direction of transforms\n",
    "    dir_sobel = np.arctan2(abs_sobely,abs_sobelx)\n",
    "    #magnitude thresholding\n",
    "    mag_thres_sobel = np.zeros_like(mag_scaled_sobel)\n",
    "    mag_thres_sobel[(mag_scaled_sobel >= mag_threshold[0]) & (mag_scaled_sobel < mag_threshold[1])] = 1\n",
    "    #direction thresholding\n",
    "    dir_thres_sobel = np.zeros_like(dir_sobel)\n",
    "    dir_thres_sobel[(dir_sobel >= dir_threshold[0]) & (dir_sobel < dir_threshold[1])] = 1\n",
    "    #combining 2 thresholdings\n",
    "    final_sobel = np.zeros_like(img)\n",
    "    final_sobel[(mag_thres_sobel == 1) & (dir_thres_sobel == 1)] = 1\n",
    "   ```\n",
    "   \n",
    "   where threshold for magnitude was set `[20,255]` and threshold for direction was set `[0,0.5]` . /\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/sobel_transformed.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "### Step 7: Sliding window polyfit for Lane find\n",
    "\n",
    "At this point I was able to use the combined binary image to isolate only the pixels belonging to lane lines. The next step was to fit a polynomial to each lane line, which was done by:\n",
    "\n",
    "* Identifying peaks in a histogram of the image to determine location of lane lines.\n",
    "* Identifying all non zero pixels around histogram peaks using the numpy function numpy.nonzero().\n",
    "* Fitting a polynomial to each lane using the numpy function numpy.polyfit().\n",
    "\n",
    "This can be done by sliding window polyfit method where you select the non-zero pixel without knowing the previous location of lanes. \n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/sliding_window.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "### Step 8: Previous frame usage\n",
    "\n",
    "If you know the previous loaction of lane lines a search within a given area around those lanes helps in finding lines continuously and computationally faster than rectangular area search.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/previous_polyfit.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "### Step 9: Radius of Curvature and Distance from center of Lane calculation\n",
    "\n",
    "After fitting the polynomials I was able to calculate the radius of curvature and position of the vehicle with respect to center with the following calculations:\n",
    "\n",
    "* Calculated the radius of curvature of left and right lane given polynomial coefficients = (1+(2A+B)^2)^1.5)/|2A|  where A. B are coefficients of equation Ax^2 + Bx + C \n",
    "* Calculated lane curvature as average curvature of both lanes.\n",
    "* Calculated the average of the x intercepts from each of the two polynomials position = (rightx_int+leftx_int)/2\n",
    "* Calculated the distance from center by taking the absolute value of the vehicle position minus the halfway point along the horizontal axis distance_from_center = abs(image_width/2 - position)\n",
    "* If the horizontal position of the car was greater than image_width/2 than the car was considered to be left of center, otherwise right of center.\n",
    "* Finally, the distance from center was converted from pixels to meters by multiplying the number of pixels by 3.7/400 in x-direction and 3.048/100 in y-direction.\n",
    "\n",
    "### Step 10: Inverting Perspective and marking Lanes on original image\n",
    "\n",
    "It's better to mark the lanes on the warped image and create a mask of lmarked up lane, and then change the perspective of the mask using perspective transform inverse matrix and add it to the original image.\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/original_image_marked.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "### Step 11: Write Radius of Curvature and Distance from Center on image\n",
    "\n",
    "<figure>\n",
    " <img src=\"test_images_output/final_image.jpg\" width=\"800\" alt=\"Combined Image\" />\n",
    "</figure>\n",
    "\n",
    "### Step 12: Define class for saving lane data\n",
    "\n",
    "I created a class for saving the data like recent_fit, best_fit and current_fit of a lane in the following way:\n",
    "\n",
    "```python\n",
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None\n",
    "        #polynomial coefficients of the last n fits of the line\n",
    "        self.recent_fit = []\n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = []  \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "\n",
    "        \n",
    "    def add_fit_coeff(self, fit):\n",
    "        if fit is not None and fit is not [0,0,450] and fit is not [0,0,830]:\n",
    "            if self.best_fit is not None:\n",
    "                self.diffs = np.absolute(fit - self.best_fit)\n",
    "            if self.diffs[0] > 0.01 or self.diffs[1] > 1 or self.diffs[2] > 100 :\n",
    "                self.detected = False\n",
    "                if len(self.recent_fit) > 0:\n",
    "                    self.current_fit = self.recent_fit[len(self.recent_fit)-1]\n",
    "                else:\n",
    "                    self.current_fit = fit\n",
    "            else:\n",
    "                self.detected = True\n",
    "                self.current_fit = fit\n",
    "                if len(self.recent_fit) != 0:\n",
    "                    if len(self.recent_fit) > 5:\n",
    "                        self.recent_fit = np.append(self.recent_fit,[fit], axis =0)\n",
    "                        self.recent_fit = self.recent_fit[len(self.recent_fit)-5:]\n",
    "                    else:\n",
    "                        self.recent_fit = np.append(self.recent_fit,[fit], axis =0)\n",
    "                    self.best_fit = np.average(self.recent_fit, axis=0)\n",
    "                else:\n",
    "                    self.recent_fit = [fit]\n",
    "        \n",
    "        else:\n",
    "            self.detected = False\n",
    "            if len(self.recent_fit) > 0:\n",
    "                # throw out oldest fit\n",
    "                self.recent_fit = self.recent_fit[1:len(self.recent_fit)]\n",
    "            if len(self.recent_fit) > 0:\n",
    "                # if there are still any fits in the queue, best_fit is their average\n",
    "                self.current_fit = np.average(self.recent_fit, axis=0)\n",
    "                self.best_fit = np.average(self.recent_fit, axis=0)\n",
    "            elif self.best_fit is not None:\n",
    "                self.current_fit = self.best_fit\n",
    "            else:\n",
    "                self.current_fit = None\n",
    "\n",
    "        \n",
    "    def add_xvalues(self, values):\n",
    "        if len(self.recent_xfitted) != 0:\n",
    "            if len(self.recent_xfitted)>5:\n",
    "                self.recent_xfitted = np.hstack([self.recent_xfitted, values])\n",
    "                self.recent_xfitted = self.current_fit[len(self.current_fit)-5:]\n",
    "            else:\n",
    "                self.recent_xfitted = np.hstack([self.recent_xfitted, values])\n",
    "            self.bestx = np.average(self.recent_xfitted, axis=0)\n",
    "            \n",
    "        else:\n",
    "            self.recent_xfitted = values\n",
    "        \n",
    "    ```\n",
    "    \n",
    "    I also added curve radius and lane positions to the class but, saving it and using the data for future purpose gives somewhat the same result as without them. So I dropped it. However, it can be added if you want to run diagnostics on the data you had and how your pipeline processed it. \n",
    "    \n",
    "### Step 13: Final Pipeline for image processing\n",
    "\n",
    "```python\n",
    "objpoints , imgpoints = calibration_point()\n",
    "\n",
    "#final pipeline for lane finding\n",
    "def pipeline(img):\n",
    "    original_img = np.copy(img)\n",
    "    binary_img, Minv = preprocessing_pipeline(img)\n",
    "    \n",
    "    #if lanes were detected in previous frame, then use search_around_poly else, use sliding_window_polyfit_generator\n",
    "    if not left.detected or not right.detected :\n",
    "        left_fit, right_fit = sliding_window_polynomial(binary_img)\n",
    "    else:\n",
    "        left_fit, right_fit = search_around_polynomial(binary_img, left.current_fit, right.current_fit)\n",
    "    \n",
    "    # invalidate both fits if the difference in their x-intercepts isn't around 350 px (+/- 100 px)\n",
    "    if left_fit is not None and right_fit is not None:\n",
    "        # calculate x-intercept (bottom of image, x=image_height) for fits\n",
    "        h = img.shape[0]\n",
    "        left_fit_x = left_fit[0]*h**2 + left_fit[1]*h + left_fit[2]\n",
    "        right_fit_x = right_fit[0]*h**2 + right_fit[1]*h + right_fit[2]\n",
    "        bottom_x_diff = abs(right_fit_x-left_fit_x)\n",
    "        if abs(350 - bottom_x_diff) > 100:\n",
    "            left_fit = None\n",
    "            right_fit = None\n",
    "            \n",
    "            \n",
    "    left.add_fit_coeff(left_fit)\n",
    "    right.add_fit_coeff(right_fit)\n",
    "    \n",
    "    # draw the current fit if it exists\n",
    "    \n",
    "    img_out1 = original_image_lane_marker(original_img, binary_img, left.current_fit, right.current_fit, Minv)\n",
    "    radius, center, left_fitx, right_fitx = radius_central_distance_calculator(img, left.current_fit, right.current_fit)\n",
    "    left.add_xvalues(left_fitx)\n",
    "    right.add_xvalues(right_fitx)\n",
    "    img_out = image_writer(img_out1, radius, center)\n",
    "    return img_out\n",
    "                ```\n",
    "                \n",
    "                \n",
    "## Results on Video Feeds\n",
    "\n",
    "Video feeds for the 3 challenges given in the project are linked below. Drawing lines throughout snaps of the video wherever the pipeline returns a marked up image helps in creating lane lines on the video as a whole. This can be used for finding lane lines in an online fashion.\n",
    "\n",
    "[Project Video](project_video_output.mp4)\n",
    "\n",
    "[Challenge Video](challenge_video_output.mp4)\n",
    "\n",
    "[Harder Challenge Video](harder_challenge_video_output.mp4)\n",
    "\n",
    "## Potential Shortcomings\n",
    "\n",
    "1. The pipeline fails to detect lanes when the lighting condition varies. Say in case there is a shadow of a tree or a flyover overhead, both cause issues with perspective transform and hence hampers the pipeline on the whole.\n",
    "\n",
    "2. The pipeline fails to detect lanes when the lane lines are changing fast, say you are driving through a mountaneous terrain and lanes curve left and right in matter of seconds. The piepline fails in such case as can be seen in harder_challenge_video.\n",
    "\n",
    "3. We have assumed a transformation matrix in perspective transform using predefined set of 4 points which is approximately the entire length of road given the elevation of road is horizontal. However, when the elevation of road varies significantly perspective transform will not be accurate and thus lane finding will be hampered.\n",
    "\n",
    "4. What is there is no lane line on roads, say streets. You cant let the car drive on its own in a small locality.\n",
    "\n",
    "\n",
    "## Possible Improvements\n",
    "\n",
    "1. Dynamic region selection based on gradient of road can be implemented to only keep lane lines in region of interest for the pipeline.\n",
    "\n",
    "2. Advanced algorithms to deal with curves along the road should be implemented to make the pipeline more generic.\n",
    "\n",
    "3. Better way to use computer vision to detect lanes based on color selection even with varied light conditions can be used if they exist.\n",
    "\n",
    "4. Advanced sensors like radars, lidars and sensor fusion can be used to cover a large area of conditional states during driving a car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
